{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e902818",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66e28347",
   "metadata": {},
   "source": [
    "Web Scraping se conoce como uno de los métodos más importantes para recuperar contenidos y datos de un sitio web automáticamente utilizando software. Esta información más tarde se puede utilizar para añadir contenido en una base de datos, exportar información a tipos de documentos estructurados, etcétera.\n",
    "\n",
    "El listado de lo que podemos \"scrapear\" en la web es amplio, pero incluye:\n",
    "- Redes sociales (Facebook, Twitter...).\n",
    "- Motores de búsqueda (Google, Bing...).\n",
    "- Páginas corporativas: tiendas online, servicios, de información empresarial, etcétera.\n",
    "- Páginas gubernamentales oficiales y de noticias.\n",
    "\n",
    "Existen dos formas de \"scrapear\", dependiendo de lo que queramos obtener de Internet:\n",
    "\n",
    "1. Obtener archivos/documentos.\n",
    "2. Obtener información.\n",
    "\n",
    "La diferencia entre el primer y segundo punto es que un archivo contiene información pero no está descrito en la página web. Con el segundo punto lo que buscamos es extraer párrafos, títulos, cantidades, importes, etcétera inmersos en la web.\n",
    "\n",
    "Como es evidente, utilizaremos `Python` para obtener contenido de Internet. Manteniendo el uso del mismo lenguaje aseguramos que todo el proceso de `ETL` quede integrado aumentando legibilidad y mantenibilidad."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5023e1ac",
   "metadata": {},
   "source": [
    "### 1. Obtener archivos/documentos\n",
    "En Python, el paquete `requests` permite interactuar con URIs HTTP y que posibilita, por ejemplo, descargar recursos y archivos alojados en alguna página web. La función que permite hacer esto es `get` y, en nuestro caso, permitiría descargar cierta información y transformarla, por ejemplo, en un DataFrame de Pandas.\n",
    "\n",
    "#### Paso 1. Encontrar el recurso a descargar\n",
    "En este caso estamos interesados en descargar información sobre los ingresos en Estados Unidos. Para ello, dado que no disponemos de información en nuestra base de datos (está vacía) buscamos recursos en Internet. Localizamos una fuente que nos podría permitir desarrollar un modelo predictivo, y accedemos a ella:\n",
    "\n",
    "![Scraping files step 1](https://github.com/4GeeksAcademy/machine-learning-content/blob/master/assets/scraping_web_files_step1.png?raw=true)\n",
    "\n",
    "#### Paso 2. Localizar el punto de descarga del recurso\n",
    "El siguiente paso es localizar desde qué dirección podremos descargar el recurso. El UCI Repository proporciona una interfaz muy intuitiva para descargar recursos. Copiando la dirección del botón `Download` podremos obtener fácilmente el punto de descarga. Sin embargo, dependiendo de la página web a veces obtener este enlace es lo más complicado de todo el proceso.\n",
    "\n",
    "Tras analizar la web, obtenemos que el enlace de descarga es el siguiente: `https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data`.\n",
    "\n",
    "![Scraping files step 2](https://github.com/4GeeksAcademy/machine-learning-content/blob/master/assets/scraping_web_files_step2.png?raw=true)\n",
    "\n",
    "#### Paso 3. Programar la descarga del fichero\n",
    "Lo último que queda antes de poder trabajar con la información es descargarla. Para ello utilizaremos el paquete `requests` ya que porporciona un mecanismo muy sencillo de utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9972e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Seleccionar el recurso a descargar\n",
    "resource_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "\n",
    "# Petición para descargar el fichero de Internet\n",
    "response = requests.get(resource_url)\n",
    "\n",
    "# Si la petición se ha ejecutado correctamente (código 200), entonces el fichero se ha podido descargar\n",
    "if response:\n",
    "    # Se almacena el archivo en el directorio actual para usarlo más tarde\n",
    "    with open(\"adult.csv\", \"wb\") as dataset:\n",
    "        dataset.write(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfdf6df3",
   "metadata": {},
   "source": [
    "El resultado es un fichero totalmente utilizable en nuestro directorio y que proviene de Internet, totalmente utilizable para el resto de pasos a realizar para entrenar nuestro modelo de Machine Learning.\n",
    "\n",
    "![Scraping files step 3](https://github.com/4GeeksAcademy/machine-learning-content/blob/master/assets/scraping_web_files_step3.png?raw=true)\n",
    "\n",
    "Ahora, podríamos leerlo con `Pandas` y crear un DataFrame a partir del fichero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63aec32e",
   "metadata": {},
   "source": [
    "### 2. Obtener información\n",
    "\n",
    "Obtener información es un proceso más tedioso que el anterior, porque necesitamos profundizar en la estructura `HTML` de los documentos para obtener esa información. Existen muchas formas de llevar a cabo este proceso, así como muchas herramientas y paquetes en Python que nos posibilitan hacerlo. `requests` y `BeautifulSoup` son una buena combinación para llevar a cabo esta tarea satisfactoriamente y de la manera más sencilla.\n",
    "\n",
    "#### Paso 1. Encontrar el contenido a obtener\n",
    "En este caso, estamos interesados en obtener la información sobre el piso más barato que se puede adquirir en Guardamar del Segura, una ciudad costera al sur de Alicante. Dado que no tenemos información al respecto, decidimos buscarla en un portal inmobiliario como Idealista. El primer paso es acceder a la web, filtrar por la ciudad y ordenar los resultados en la web:\n",
    "\n",
    "![Scraping content step 1](https://github.com/4GeeksAcademy/machine-learning-content/blob/master/assets/scraping_web_content_step1.png?raw=true)\n",
    "\n",
    "Después de filtrar el contenido y prepararlo (esto se hace así con un ejemplo sencillo, pero normalmente se filtran y ordenan resultados a nivel código), obtenemos la URL del contenido general, sobre el que obtendremos el importe total: `https://www.idealista.com/buscar/venta-viviendas/guardamar-del-segura-alicante/guardamar/?ordenado-por=precios-asc`\n",
    "\n",
    "#### Paso 2. Descargar todo el contenido HTML de la URL\n",
    "A continuación debemos descargar el contenido de la página anterior. Para ello, utilizamos por un lado la librería `requests` para descargar el HTML en formato de texto plano, y `BeautifulSoup` para generar el árbol de elementos y poder realizar consultas para obtener la información que queramos obtener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c143cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Seleccionar el recurso a descargar\n",
    "resource_url = \"https://inmobiliariacgi.com/es/component/jak2filter/?Itemid=111&issearch=1&category_id=1,3&theme=propiedades&isc=1&ordering=zdate&xf_3=2&orders[xf5]=xf5\"\n",
    "\n",
    "# Petición para descargar el fichero de Internet\n",
    "response = requests.get(resource_url, time.sleep(10))\n",
    "\n",
    "# Si la petición se ha ejecutado correctamente (código 200), entonces el contenido HTML de la página se ha podido descargar\n",
    "if response:\n",
    "    # Transformamos el HTML plano en un HTML real (estructurado y anidado, con forma de árbol)\n",
    "    soup = BeautifulSoup(response.text, 'html')\n",
    "    soup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d1c7d4d",
   "metadata": {},
   "source": [
    "Como se puede apreciar, el objeto `soup` contiene el HTML y a partir de él se pueden hacer ciertas consultas para obtener la información. En este caso buscamos obtener el importe (marcado en rojo en la imagen). Para poder llevar a cabo una extracción satisfactoria, debemos buscar el elemento en el HTML de la página web antes de comenzar a trabajar con el objeto `soup`. Para ello, nos servimos de las [herramientas de desarrollador](https://developer.chrome.com/docs/devtools/overview/) de nuestro navegador:\n",
    "\n",
    "![Scraping content step 2](https://github.com/4GeeksAcademy/machine-learning-content/blob/master/assets/scraping_web_content_step2.png?raw=true)\n",
    "\n",
    "Buscando el elemento lo encontramos encerrado dentro del siguiente `div`:\n",
    "\n",
    "```\n",
    "<div class=\"precio\">\n",
    "    <span class=\"preciodestacadas\">47.975 €</span>\n",
    "</div>\n",
    "```\n",
    "\n",
    "Además, como se aprecia en la flecha verde, este elemento anterior se encuentra encerrado dentro de una jerarquía que desencadena el elemento `body` del HTML. Existen dos formas de extraer el valor que queremos:\n",
    "\n",
    "1. Filtro por jerarquía.\n",
    "2. Filtro por nombre de etiqueta.\n",
    "3. Filtro por atributos.\n",
    "\n",
    "##### Filtro por jerarquía\n",
    "Este tipo de filtro requiere de recorrer todo el árbol jerárquico del HTML hasta dar con el elemento. Sabemos que tras haber encontrado el elemento, observando la imagen anterior, la jerarquía es la siguiente:\n",
    "\n",
    "`body > main > div > div > div > div > div > div > div > div > span`\n",
    "\n",
    "Y siendo el elemento `span` el que contiene el importe que queremos extraer. Sin embargo, esta forma de extraer información es muy poco eficiente, no es mantenible (pequeños cambios en la página web pueden afectar mucho a la extracción) y requiere de tiempos de desarrollo muy elevados, ya que hay páginas web que cuentan con una jerarquización todavía más compleja.\n",
    "\n",
    "##### Filtro por nombre de etiqueta\n",
    "Es uno de los filtros más comunes y utilizados. Es el filtro más básico ya que consiste en pasar el nombre de la etiqueta a buscar en la función de búsqueda, para seleccionar después el deseado.\n",
    "\n",
    "En nuestro ejemplo, estamos buscando un elemento `span`, así que no tenemos más que encontrar todos los que haya en el documento HTML y procesarlos hasta encontrar el deseado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9df0929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El importe de la casa de Guardamar del Segura más barata es <re.Match object; span=(0, 6), match='47.975'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Obtener todos los elementos de tipo 'span' del documento HTML\n",
    "spans = soup.find_all(\"span\")\n",
    "\n",
    "# Iteramos por cada uno de los resultados para encontrar el elemento que contiene el importe determinado. Como el importe que estamos buscando es el primero de todos, lo buscamos (por ejemplo, con una expresión regular) y cuando lo encontramos, imprimimos su valor\n",
    "for span in spans:\n",
    "    amount = re.search(r'\\d+\\.\\d+', span.text)\n",
    "    if amount:\n",
    "        break\n",
    "\n",
    "print(f\"El importe de la casa de Guardamar del Segura más barata es {amount}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35580d2b",
   "metadata": {},
   "source": [
    "Sin embargo, esta metodología es poco útil si, por ejemplo, hubiese otro importe encima que no estuviese relacionado con lo que queremos extraer. Además, es muy poco eficiente en tiempo ya que requiere que se analicen todos los elementos de la web hasta encontrar el apropiado. Esto provoca que sea una mala alternativa para entornos en tiempo real, grandes análisis, etcétera.\n",
    "\n",
    "##### Filtro por atributos\n",
    "Podemos utilizar otro mecanismo para seleccionar elementos de nuestro árbol HTML: `identificadores` y `clases`. De esta forma, podemos localizar rápidamente el elemento a través de su clase. Así, si el `div` que contenía el `span` era el siguiente:\n",
    "\n",
    "```\n",
    "<div class=\"precio\">\n",
    "    <span class=\"preciodestacadas\">47.975 €</span>\n",
    "</div>\n",
    "```\n",
    "\n",
    "Entonces usando la clase `preciodestacadas` podríamos filtrar rápidamente este elemento y obtenerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b55ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El importe de la casa de Guardamar del Segura más barata es 47.975 €\n"
     ]
    }
   ],
   "source": [
    "amounts = soup.find_all(\"span\", class_=\"preciodestacadas\")\n",
    "\n",
    "print(f\"El importe de la casa de Guardamar del Segura más barata es {amounts[0].text}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2932bf75",
   "metadata": {},
   "source": [
    "Para obtener el texto de un elemento `span`, debemos utilizar el atributo `text`, tal y como se aprecia en el código anterior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
