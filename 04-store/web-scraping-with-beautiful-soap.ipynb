{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e902818",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e28347",
   "metadata": {},
   "source": [
    "Web Scraping is one of the important methods to retrieve data from a website automatically. Not all websites allow people to scrape, however you can add the ‘robots.txt’ after the URL of the website you want to scrape, in order to know whether you will be allowed to scrape or not. \n",
    "\n",
    "How do we get data from a website?\n",
    "\n",
    "There are three ways in which we can get data from the web:\n",
    "\n",
    "1. Importing files from the internet.\n",
    "\n",
    "2. Doing web scraping directly with code to download the HMTL content.\n",
    "\n",
    "3. Querying data from the website API.\n",
    "\n",
    "But what is a website API?\n",
    "\n",
    "An API (Application Programming Interface) is a software interface that allows two applications to interact with each other without any user intervention. A web API can be accesed over the web using the HTTP protocol.\n",
    "\n",
    "Scraping tools are specially developed software to extract data from websites. Which are the most common tools for web scraping?\n",
    "\n",
    "- Requests: It is a Python module in which we can send HTTP requests to retrieve contents. It helps us access website HTML contents or API by sending Get or Post requests.\n",
    "\n",
    "- Beautiful Soup: It helps us parse HTML or XML documents into a readable format. We can retrieve information faster.\n",
    "\n",
    "- Selenium : Mostly used for website testing. It helps automate different events.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023e1ac",
   "metadata": {},
   "source": [
    "### 1. Importing flat files from the web\n",
    "\n",
    "The flat file we will import will be the iris dataset from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/ obtained from the UCI Machine Learning Repository.\n",
    "\n",
    "After importing it, we will load it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf2c98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5.1,3.5,1.4,0.2,Iris-setosa\n",
      "0  4.9,3.0,1.4,0.2,Iris-setosa\n",
      "1  4.7,3.2,1.3,0.2,Iris-setosa\n",
      "2  4.6,3.1,1.5,0.2,Iris-setosa\n",
      "3  5.0,3.6,1.4,0.2,Iris-setosa\n",
      "4  5.4,3.9,1.7,0.4,Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url,'iris.csv')\n",
    "\n",
    "# Read file in a dataframe and look at the first rows\n",
    "df = pd.read_csv('iris.csv', sep=';')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43ce9f",
   "metadata": {},
   "source": [
    "### 2. Performing HTTP requests\n",
    "\n",
    "Steps to do HTTP requests:\n",
    "\n",
    "1. Inspect the website HTML that we want to scrape (right-click)\n",
    "\n",
    "2. Access URL of the website using code and download all the HTML contents on the page\n",
    "\n",
    "3. Format the downloaded content into a readable format\n",
    "\n",
    "4. Extract out useful information and save it into a structured format\n",
    "\n",
    "5. If the information is in multiple pages of the website, we may need to repeat steps 2–4 to have the complete information.\n",
    "\n",
    "**Performing HTTP requests using urllib**\n",
    "\n",
    "\n",
    "We'll extract the HTML itself, but first we are going to package and send the request and then catch the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77984bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \" https://scikit-learn.org/stable/getting_started.html\"\n",
    "\n",
    "# This packages the request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catch the response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Close the response!\n",
    "response.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ad6f5",
   "metadata": {},
   "source": [
    "This response is a http.client.HTTPResponse object. What can we do with it?\n",
    "\n",
    "As it came from an HTML page, we can read it to extract the HTML using a read() method associated to it. Now let's extract the response and print the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9610a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = Request(url)\n",
    "\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extract the response: html\n",
    "html = response.read()\n",
    "\n",
    "# Print the html\n",
    "print(html)\n",
    "\n",
    "# Close the response!\n",
    "response.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1401b",
   "metadata": {},
   "source": [
    "**Performing HTTP requests using requests**\n",
    "\n",
    "Now we are going to use the requests library. This time we don't have to close the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Specify the url: url\n",
    "url = \"https://scikit-learn.org/stable/getting_started.html\"\n",
    "\n",
    "# Packages the request, send the request and catch the response: resp\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Extract the response: text\n",
    "text = resp.text\n",
    "\n",
    "# Print the html\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe2056",
   "metadata": {},
   "source": [
    "**Parsing HTML Using Beautiful Soup**\n",
    "\n",
    "We'll learn how to use the BeautifulSoup package to parse, prettify and extract information from HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://gvanrossum.github.io//'\n",
    "\n",
    "# Package the request, send the request and catch the response: resp\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = resp.text\n",
    "\n",
    "#Then, all we have to do is convert the HTML document to a BeautifulSoup object!\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the response\n",
    "print(pretty_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa08e2",
   "metadata": {},
   "source": [
    "**Tags can be called in different ways**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2810657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line of code creates a BeautifulSoup object from a webpage:\n",
    " \n",
    "soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    " \n",
    "# Within the `soup` object, tags can be called by name:\n",
    " \n",
    "first_div = soup.div\n",
    " \n",
    "# or by CSS selector:\n",
    " \n",
    "all_elements_of_header_class = soup.select(\".header\")\n",
    " \n",
    "# or by a call to `.find_all`:\n",
    " \n",
    "all_p_elements = soup.find_all(\"p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd5f1b",
   "metadata": {},
   "source": [
    "### 3. Interacting with APIs\n",
    "\n",
    "it is a bit more complicated than scraping the HTML document, especially if authentication is required, but the data will be more structured and stable.\n",
    "\n",
    "Steps to querying data from the website API:\n",
    "\n",
    "1. Inspect the XHR network section of the URL that we want to scrape\n",
    "\n",
    "2. Find out the request-response that gives us the data that we want\n",
    "\n",
    "3. Depending on the type of request(post or get), let's simulate the request in our code and retrieve the data from API. If authentication is required, we will need to request for token first before sending our POST request\n",
    "\n",
    "4. Extract useful information that we need\n",
    "\n",
    "5. For API with a limit on query size, we will need to use ‘for loop’ to repeatedly retrieve all the data\n",
    "\n",
    "**Example: Loading and exploring a Json with GET request**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce3a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = \"https://covid-19-statistics.p.rapidapi.com/regions\"\n",
    "\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Host\": \"covid-19-statistics.p.rapidapi.com\",\n",
    "\t\"X-RapidAPI-Key\": \"SIGN-UP-FOR-KEY\"\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = response.json()\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a0b233",
   "metadata": {},
   "source": [
    "If you want to scrape a website, we should check the existence of API first in the network section using inspect. If we can find the response to a request that gives us all the data we need, we can build a stable solution. If we cannot find the data in-network, we can try using requests or Selenium to download HTML content and use Beautiful Soup to format the data.\n",
    "\n",
    "Other top web scraping tools in 2022:\n",
    "\n",
    "1. Newsdata.io\n",
    "\n",
    "2. Scrapingbee \n",
    "\n",
    "3. Bright Data\n",
    "\n",
    "4. Scraping-bot \n",
    "\n",
    "5. Scraper API \n",
    "\n",
    "6. Scrapestack \n",
    "\n",
    "7. Apify \n",
    "\n",
    "8. Agenty \n",
    "\n",
    "9. Import.io\n",
    "\n",
    "10. Outwit \n",
    "\n",
    "11. Webz.io "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1945b8a8",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "https://towardsdatascience.com/web-scraping-basics-82f8b5acd45c\n",
    "\n",
    "https://rapidapi.com/rapidapi/api\n",
    "\n",
    "https://newsdata.io/blog/top-21-web-scraping-tools-for-you/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
